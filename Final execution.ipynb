{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 02:11:55,617 - INFO - ✅ Connected to Neo4j database\n",
      "2025-03-01 02:11:55,643 - INFO - ✅ Initialized Gemini LLM\n",
      "C:\\Users\\SOHAM\\AppData\\Local\\Temp\\ipykernel_31972\\124282538.py:46: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ecommerce_df = pd.read_csv(ecommerce_path)\n",
      "2025-03-01 02:11:57,968 - INFO - Loaded e-commerce data: (128975, 24)\n",
      "2025-03-01 02:11:57,982 - INFO - Loaded chat logs: (1057, 1)\n",
      "2025-03-01 02:12:02,137 - INFO - Cleared existing database\n",
      "2025-03-01 02:12:02,327 - WARNING - Index creation warning: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input ')': expected ':' (line 1, column 50 (offset: 49))\n",
      "\"CREATE INDEX entity_id_index IF NOT EXISTS FOR (n) ON (n.id)\"\n",
      "                                                  ^}\n",
      "2025-03-01 02:12:02,367 - WARNING - Index creation warning: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input ')': expected ':' (line 1, column 52 (offset: 51))\n",
      "\"CREATE INDEX entity_name_index IF NOT EXISTS FOR (n) ON (n.name)\"\n",
      "                                                    ^}\n",
      "2025-03-01 02:12:02,399 - WARNING - Index creation warning: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input ')': expected ':' (line 1, column 52 (offset: 51))\n",
      "\"CREATE INDEX entity_type_index IF NOT EXISTS FOR (n) ON (n.entity_type)\"\n",
      "                                                    ^}\n",
      "2025-03-01 02:12:02,431 - WARNING - Index creation warning: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input ']': expected ':' (line 1, column 61 (offset: 60))\n",
      "\"CREATE INDEX relationship_type_index IF NOT EXISTS FOR ()-[r]-() ON TYPE(r)\"\n",
      "                                                             ^}\n",
      "2025-03-01 02:12:02,433 - INFO - Created basic indices\n",
      "2025-03-01 02:12:11,051 - ERROR - Failed to parse LLM schema analysis as JSON\n",
      "2025-03-01 02:12:11,053 - INFO - Using fallback schema creation\n",
      "2025-03-01 02:14:17,634 - INFO - Created basic fallback schema\n",
      "2025-03-01 02:14:17,937 - WARNING - Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: $label)} {position: line: 3, column: 30, offset: 79} for query: '\\n                    CALL db.labels() YIELD label\\n                    MATCH (n:`$label`)\\n                    RETURN label, count(n) AS count\\n                    ORDER BY count DESC\\n                '\n",
      "2025-03-01 02:14:29,487 - WARNING - Failed to parse enrichment suggestions as JSON\n",
      "2025-03-01 02:14:29,492 - INFO - ✅ Successfully converted data to knowledge graph\n",
      "2025-03-01 02:14:29,493 - INFO - Processing query: Give 5 names of products which sold last by date present in dataset?\n",
      "2025-03-01 02:14:32,664 - INFO - Generated Cypher query: MATCH (p:Product)\n",
      "RETURN p.id, p.Date\n",
      "ORDER BY p.Date DESC \n",
      "LIMIT 5\n",
      "2025-03-01 02:14:33,123 - INFO - Found 5 results in knowledge graph\n",
      "2025-03-01 02:14:40,043 - INFO - Query result: Based on the knowledge graph, here's what I found: [{'p.id': 'J0062-DR-M', 'p.Date': '06-29-22'}, {'p.id': 'SET414-KR-NP-XXXL', 'p.Date': '06-29-22'}, {'p.id': 'JNE3486-KR-XXL', 'p.Date': '06-29-22'}, {'p.id': 'SET444-KR-SH-M', 'p.Date': '06-29-22'}, {'p.id': 'SET444-KR-SH-L', 'p.Date': '06-29-22'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTS ===\n",
      "\n",
      "Query: Give 5 names of products which sold last by date present in dataset?\n",
      "Answer: Based on the knowledge graph, here's what I found: [{'p.id': 'J0062-DR-M', 'p.Date': '06-29-22'}, {'p.id': 'SET414-KR-NP-XXXL', 'p.Date': '06-29-22'}, {'p.id': 'JNE3486-KR-XXL', 'p.Date': '06-29-22'}, {'p.id': 'SET444-KR-SH-M', 'p.Date': '06-29-22'}, {'p.id': 'SET444-KR-SH-L', 'p.Date': '06-29-22'}]\n",
      "Insights: The knowledge graph provided the above information based on your query.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 25 Feb Main Code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "import google.generativeai as genai\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import json\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GraphRAGSystem:\n",
    "    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str, gemini_api_key: str):\n",
    "        \"\"\"Initialize the Graph RAG system with necessary connections\"\"\"\n",
    "        self.setup_database(neo4j_uri, neo4j_user, neo4j_password)\n",
    "        self.setup_llm(gemini_api_key)\n",
    "        self.last_api_call = 0\n",
    "        self.min_delay = 2.0  # Minimum delay between API calls\n",
    "        self.knowledge_base = {}  # Store extracted knowledge\n",
    "\n",
    "    def setup_database(self, uri: str, user: str, password: str) -> None:\n",
    "        \"\"\"Setup Neo4j connection\"\"\"\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        logger.info(\"✅ Connected to Neo4j database\")\n",
    "\n",
    "    def setup_llm(self, api_key: str) -> None:\n",
    "        \"\"\"Setup Gemini LLM\"\"\"\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-1.5-pro\",\n",
    "            generation_config={\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_output_tokens\": 800,\n",
    "            }\n",
    "        )\n",
    "        self.chat = self.model.start_chat(history=[])\n",
    "        logger.info(\"✅ Initialized Gemini LLM\")\n",
    "\n",
    "    def load_datasets(self, ecommerce_path: str, chat_logs_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load both datasets and perform initial preprocessing\"\"\"\n",
    "        try:\n",
    "            # Load e-commerce data\n",
    "            ecommerce_df = pd.read_csv(ecommerce_path)\n",
    "            logger.info(f\"Loaded e-commerce data: {ecommerce_df.shape}\")\n",
    "\n",
    "            # Load chat logs\n",
    "            chat_logs_df = pd.read_csv(chat_logs_path)\n",
    "            logger.info(f\"Loaded chat logs: {chat_logs_df.shape}\")\n",
    "\n",
    "            # Store original dataframes for later use\n",
    "            self.ecommerce_df = ecommerce_df\n",
    "            self.chat_logs_df = chat_logs_df\n",
    "            \n",
    "            # Convert data to knowledge graph\n",
    "            self._convert_to_knowledge_graph(ecommerce_df, chat_logs_df)\n",
    "\n",
    "            return ecommerce_df, chat_logs_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading datasets: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _convert_to_knowledge_graph(self, ecommerce_df: pd.DataFrame, chat_logs_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Convert data directly to knowledge graph without predefined structure\"\"\"\n",
    "        try:\n",
    "            # First, clear existing database\n",
    "            self._clear_database()\n",
    "            \n",
    "            # Create basic constraints and indices for performance\n",
    "            self._create_basic_indices()\n",
    "            \n",
    "            # Extract entities and relationships using LLM\n",
    "            self._discover_entities_relationships(ecommerce_df, chat_logs_df)\n",
    "            \n",
    "            # Create additional relationships and insights\n",
    "            self._enrich_knowledge_graph()\n",
    "            \n",
    "            logger.info(\"✅ Successfully converted data to knowledge graph\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting to knowledge graph: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _clear_database(self) -> None:\n",
    "        \"\"\"Clear existing graph database\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            logger.info(\"Cleared existing database\")\n",
    "\n",
    "    def _create_basic_indices(self) -> None:\n",
    "        \"\"\"Create basic indices without restricting node types\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Create some basic indices for common properties\n",
    "            indices = [\n",
    "                \"CREATE INDEX entity_id_index IF NOT EXISTS FOR (n) ON (n.id)\",\n",
    "                \"CREATE INDEX entity_name_index IF NOT EXISTS FOR (n) ON (n.name)\",\n",
    "                \"CREATE INDEX entity_type_index IF NOT EXISTS FOR (n) ON (n.entity_type)\",\n",
    "                \"CREATE INDEX relationship_type_index IF NOT EXISTS FOR ()-[r]-() ON TYPE(r)\"\n",
    "            ]\n",
    "            \n",
    "            for idx in indices:\n",
    "                try:\n",
    "                    session.run(idx)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Index creation warning: {str(e)}\")\n",
    "            \n",
    "            logger.info(\"Created basic indices\")\n",
    "\n",
    "    def _discover_entities_relationships(self, ecommerce_df: pd.DataFrame, chat_logs_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Use LLM to discover entities and relationships in the data\"\"\"\n",
    "        # First, analyze the dataframes to understand their structure\n",
    "        ecommerce_columns = ecommerce_df.columns.tolist()\n",
    "        chat_columns = chat_logs_df.columns.tolist()\n",
    "        \n",
    "        # Get sample data\n",
    "        ecommerce_sample = ecommerce_df.head(5).to_dict('records')\n",
    "        chat_sample = chat_logs_df.head(5).to_dict('records')\n",
    "        \n",
    "        # Use LLM to identify entities and relationships\n",
    "        schema_prompt = f\"\"\"\n",
    "        Analyze these datasets to identify entities and relationships for a knowledge graph:\n",
    "        \n",
    "        E-commerce Dataset Columns: {ecommerce_columns}\n",
    "        E-commerce Sample Data: {ecommerce_sample}\n",
    "        \n",
    "        Chat Logs Dataset Columns: {chat_columns}\n",
    "        Chat Logs Sample Data: {chat_sample}\n",
    "        \n",
    "        Please identify:\n",
    "        1. All potential entity types (nodes) in this data\n",
    "        2. Key properties for each entity type\n",
    "        3. Potential relationships between entities\n",
    "        4. Complex patterns or insights that might emerge from analyzing this data\n",
    "        \n",
    "        Return your analysis as a JSON object with:\n",
    "        - \"entity_types\": a list of entity types and their key identifying properties\n",
    "        - \"relationships\": potential relationships between entities\n",
    "        - \"insights\": complex patterns or insights that could be extracted\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            schema_response = self._safe_llm_call(schema_prompt)\n",
    "            schema_analysis = json.loads(schema_response)\n",
    "            \n",
    "            # Extract entity types and relationships from LLM response\n",
    "            entity_types = schema_analysis.get('entity_types', [])\n",
    "            relationship_types = schema_analysis.get('relationships', [])\n",
    "            \n",
    "            # Process e-commerce data to create entities\n",
    "            self._process_ecommerce_data(ecommerce_df, entity_types)\n",
    "            \n",
    "            # Process chat data to create entities\n",
    "            self._process_chat_data(chat_logs_df, entity_types)\n",
    "            \n",
    "            # Create the discovered relationships\n",
    "            self._create_discovered_relationships(ecommerce_df, chat_logs_df, relationship_types)\n",
    "            \n",
    "            logger.info(f\"Created {len(entity_types)} entity types and {len(relationship_types)} relationship types\")\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            logger.error(\"Failed to parse LLM schema analysis as JSON\")\n",
    "            # Fallback: Use a simpler approach with predefined entities\n",
    "            self._fallback_schema_creation(ecommerce_df, chat_logs_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error discovering entities and relationships: {str(e)}\")\n",
    "            self._fallback_schema_creation(ecommerce_df, chat_logs_df)\n",
    "\n",
    "    def _process_ecommerce_data(self, df: pd.DataFrame, entity_types: List[Dict]) -> None:\n",
    "        \"\"\"Process e-commerce data to create entity nodes based on discovered types\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Process each entity type that can be found in the e-commerce data\n",
    "            for entity_type in entity_types:\n",
    "                type_name = entity_type.get('name')\n",
    "                id_property = entity_type.get('id_property')\n",
    "                properties = entity_type.get('properties', [])\n",
    "                \n",
    "                if not type_name or not id_property:\n",
    "                    continue\n",
    "                \n",
    "                # Check if the id_property exists in the dataframe\n",
    "                if id_property not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Find valid properties that exist in dataframe\n",
    "                valid_properties = [p for p in properties if p in df.columns]\n",
    "                \n",
    "                # Group by the ID property to create unique entities\n",
    "                unique_entities = df.groupby(id_property).first().reset_index()\n",
    "                \n",
    "                # Create entities in batches to avoid memory issues\n",
    "                batch_size = 1000\n",
    "                for i in range(0, len(unique_entities), batch_size):\n",
    "                    batch = unique_entities.iloc[i:i+batch_size]\n",
    "                    \n",
    "                    # Create a parameter list for batch processing\n",
    "                    params_list = []\n",
    "                    for _, row in batch.iterrows():\n",
    "                        entity_id = row[id_property]\n",
    "                        # Create properties dict with non-null values\n",
    "                        props = {\n",
    "                            'id': str(entity_id),\n",
    "                            'entity_type': type_name\n",
    "                        }\n",
    "                        \n",
    "                        for prop in valid_properties:\n",
    "                            if prop in row and not pd.isna(row[prop]):\n",
    "                                props[prop] = str(row[prop])\n",
    "                        \n",
    "                        params_list.append(props)\n",
    "                    \n",
    "                    # Create entities in batch\n",
    "                    if params_list:\n",
    "                        query = f\"\"\"\n",
    "                        UNWIND $params AS param\n",
    "                        MERGE (n:{type_name} {{id: param.id}})\n",
    "                        SET n += param\n",
    "                        \"\"\"\n",
    "                        session.run(query, params=params_list)\n",
    "                \n",
    "                logger.info(f\"Created {len(unique_entities)} {type_name} entities\")\n",
    "\n",
    "    def _process_chat_data(self, df: pd.DataFrame, entity_types: List[Dict]) -> None:\n",
    "        \"\"\"Process chat data to create entity nodes based on discovered types\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Process each entity type that can be found in the chat data\n",
    "            for entity_type in entity_types:\n",
    "                type_name = entity_type.get('name')\n",
    "                id_property = entity_type.get('id_property')\n",
    "                properties = entity_type.get('properties', [])\n",
    "                message_property = entity_type.get('message_property')\n",
    "                \n",
    "                # Skip entity types that don't use message data or lack required properties\n",
    "                if not type_name or not message_property or message_property not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # For chat data, we need to extract entities from messages\n",
    "                # Use LLM to detect entities in the messages\n",
    "                \n",
    "                # Sample messages for entity extraction\n",
    "                sample_messages = df[message_property].sample(min(100, len(df))).tolist()\n",
    "                \n",
    "                entity_prompt = f\"\"\"\n",
    "                Analyze these customer service messages to identify {type_name} entities:\n",
    "                \n",
    "                Messages: {sample_messages}\n",
    "                \n",
    "                Extract all instances of {type_name} entities mentioned in these messages.\n",
    "                For each entity, identify key properties: {properties}\n",
    "                \n",
    "                Return as a JSON list of entities with their properties.\n",
    "                \"\"\"\n",
    "                \n",
    "                try:\n",
    "                    entity_response = self._safe_llm_call(entity_prompt)\n",
    "                    detected_entities = json.loads(entity_response)\n",
    "                    \n",
    "                    # Create the detected entities\n",
    "                    if detected_entities and isinstance(detected_entities, list):\n",
    "                        # Prepare batch parameters\n",
    "                        params_list = []\n",
    "                        for entity in detected_entities:\n",
    "                            # Ensure the entity has a unique ID\n",
    "                            if id_property not in entity:\n",
    "                                continue\n",
    "                            \n",
    "                            entity_id = entity[id_property]\n",
    "                            entity['id'] = str(entity_id)\n",
    "                            entity['entity_type'] = type_name\n",
    "                            params_list.append(entity)\n",
    "                        \n",
    "                        # Create entities in batch\n",
    "                        if params_list:\n",
    "                            query = f\"\"\"\n",
    "                            UNWIND $params AS param\n",
    "                            MERGE (n:{type_name} {{id: param.id}})\n",
    "                            SET n += param\n",
    "                            \"\"\"\n",
    "                            session.run(query, params=params_list)\n",
    "                            \n",
    "                        logger.info(f\"Created {len(params_list)} {type_name} entities from chat data\")\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Failed to parse entity extraction for {type_name} as JSON\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error processing chat data for {type_name}: {str(e)}\")\n",
    "\n",
    "    def _create_discovered_relationships(self, ecommerce_df: pd.DataFrame, chat_df: pd.DataFrame, \n",
    "                                         relationship_types: List[Dict]) -> None:\n",
    "        \"\"\"Create relationships based on discovered patterns\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # For each relationship type discovered by the LLM\n",
    "            for rel_type in relationship_types:\n",
    "                source_type = rel_type.get('source_type')\n",
    "                target_type = rel_type.get('target_type')\n",
    "                relationship = rel_type.get('relationship')\n",
    "                source_id = rel_type.get('source_id')\n",
    "                target_id = rel_type.get('target_id')\n",
    "                dataset = rel_type.get('dataset', 'ecommerce')\n",
    "                \n",
    "                if not all([source_type, target_type, relationship, source_id, target_id]):\n",
    "                    continue\n",
    "                \n",
    "                # Select the appropriate dataset\n",
    "                df = ecommerce_df if dataset == 'ecommerce' else chat_df\n",
    "                \n",
    "                # Check if required columns exist\n",
    "                if source_id not in df.columns or target_id not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # For related entities in the same dataset\n",
    "                try:\n",
    "                    # Extract unique relationship pairs\n",
    "                    rel_data = df[[source_id, target_id]].drop_duplicates()\n",
    "                    rel_data = rel_data.dropna()\n",
    "                    \n",
    "                    # Create relationships in batches\n",
    "                    batch_size = 1000\n",
    "                    for i in range(0, len(rel_data), batch_size):\n",
    "                        batch = rel_data.iloc[i:i+batch_size]\n",
    "                        \n",
    "                        # Prepare relationship batch parameters\n",
    "                        params_list = []\n",
    "                        for _, row in batch.iterrows():\n",
    "                            source_value = str(row[source_id])\n",
    "                            target_value = str(row[target_id])\n",
    "                            params_list.append({\n",
    "                                'source_id': source_value, \n",
    "                                'target_id': target_value,\n",
    "                            })\n",
    "                        \n",
    "                        # Create relationships in batch\n",
    "                        if params_list:\n",
    "                            query = f\"\"\"\n",
    "                            UNWIND $params AS param\n",
    "                            MATCH (a:{source_type} {{id: param.source_id}})\n",
    "                            MATCH (b:{target_type} {{id: param.target_id}})\n",
    "                            MERGE (a)-[r:{relationship}]->(b)\n",
    "                            \"\"\"\n",
    "                            session.run(query, params=params_list)\n",
    "                    \n",
    "                    logger.info(f\"Created {len(rel_data)} {relationship} relationships from {source_type} to {target_type}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error creating {relationship} relationships: {str(e)}\")\n",
    "\n",
    "    def _enrich_knowledge_graph(self) -> None:\n",
    "        \"\"\"Analyze the graph to discover additional relationships and insights\"\"\"\n",
    "        try:\n",
    "            # Get summary of current graph\n",
    "            with self.driver.session() as session:\n",
    "                # Count nodes by label\n",
    "                node_count = session.run(\"\"\"\n",
    "                    CALL db.labels() YIELD label\n",
    "                    MATCH (n:`$label`)\n",
    "                    RETURN label, count(n) AS count\n",
    "                    ORDER BY count DESC\n",
    "                \"\"\").data()\n",
    "                \n",
    "                # Count relationships by type\n",
    "                rel_count = session.run(\"\"\"\n",
    "                    MATCH ()-[r]->()\n",
    "                    RETURN type(r) AS type, count(r) AS count\n",
    "                    ORDER BY count DESC\n",
    "                \"\"\").data()\n",
    "            \n",
    "            # Use LLM to identify additional insights\n",
    "            enrich_prompt = f\"\"\"\n",
    "            Analyze this knowledge graph structure to identify additional relationships and insights:\n",
    "            \n",
    "            Node counts by type: {node_count}\n",
    "            Relationship counts by type: {rel_count}\n",
    "            \n",
    "            Suggest:\n",
    "            1. Additional relationship types that could be inferred from existing ones\n",
    "            2. Potential property patterns that could be used to create new relationships\n",
    "            3. Complex graph patterns that could reveal insights (like product co-purchasing)\n",
    "            \n",
    "            Return as a JSON object with:\n",
    "            - \"new_relationships\": a list of Cypher queries to create new relationship types\n",
    "            - \"property_patterns\": a list of Cypher queries to set new properties\n",
    "            - \"complex_patterns\": a list of Cypher queries to detect and create complex patterns\n",
    "            \"\"\"\n",
    "            \n",
    "            enrich_response = self._safe_llm_call(enrich_prompt)\n",
    "            \n",
    "            try:\n",
    "                enrichment = json.loads(enrich_response)\n",
    "                \n",
    "                with self.driver.session() as session:\n",
    "                    # Execute the suggested queries to create new relationships\n",
    "                    for query in enrichment.get('new_relationships', []):\n",
    "                        try:\n",
    "                            session.run(query)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Error executing enrichment query: {str(e)}\")\n",
    "                    \n",
    "                    # Set new properties based on patterns\n",
    "                    for query in enrichment.get('property_patterns', []):\n",
    "                        try:\n",
    "                            session.run(query)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Error setting properties: {str(e)}\")\n",
    "                    \n",
    "                    # Create complex pattern-based relationships\n",
    "                    for query in enrichment.get('complex_patterns', []):\n",
    "                        try:\n",
    "                            session.run(query)\n",
    "                        except Exception as e:\n",
    "                            logger.warning(f\"Error creating complex patterns: {str(e)}\")\n",
    "                \n",
    "                logger.info(\"✅ Enriched knowledge graph with additional relationships and patterns\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                logger.warning(\"Failed to parse enrichment suggestions as JSON\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error enriching knowledge graph: {str(e)}\")\n",
    "\n",
    "    def _fallback_schema_creation(self, ecommerce_df: pd.DataFrame, chat_logs_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Create a basic schema if the LLM-based discovery fails\"\"\"\n",
    "        logger.info(\"Using fallback schema creation\")\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            # Create basic Product nodes\n",
    "            if 'SKU' in ecommerce_df.columns:\n",
    "                for _, row in ecommerce_df.drop_duplicates('SKU').iterrows():\n",
    "                    props = {'id': str(row['SKU']), 'entity_type': 'Product'}\n",
    "                    for col in ecommerce_df.columns:\n",
    "                        if col != 'SKU' and not pd.isna(row[col]):\n",
    "                            props[col] = str(row[col])\n",
    "                    \n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (p:Product {id: $id})\n",
    "                        SET p += $props\n",
    "                    \"\"\", id=props['id'], props=props)\n",
    "            \n",
    "            # Create basic Category nodes\n",
    "            if 'Category' in ecommerce_df.columns:\n",
    "                for category in ecommerce_df['Category'].dropna().unique():\n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (c:Category {id: $id, name: $name, entity_type: 'Category'})\n",
    "                    \"\"\", id=str(category), name=str(category))\n",
    "            \n",
    "            # Create product-category relationships\n",
    "            if 'SKU' in ecommerce_df.columns and 'Category' in ecommerce_df.columns:\n",
    "                for _, row in ecommerce_df[['SKU', 'Category']].dropna().drop_duplicates().iterrows():\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (p:Product {id: $sku})\n",
    "                        MATCH (c:Category {id: $category})\n",
    "                        MERGE (p)-[:BELONGS_TO]->(c)\n",
    "                    \"\"\", sku=str(row['SKU']), category=str(row['Category']))\n",
    "            \n",
    "            # Extract entities from chat messages\n",
    "            if 'message' in chat_logs_df.columns:\n",
    "                sample_messages = chat_logs_df['message'].sample(min(50, len(chat_logs_df))).tolist()\n",
    "                \n",
    "                # Create basic CustomerIssue nodes\n",
    "                issue_prompt = f\"\"\"\n",
    "                Extract customer issues from these messages:\n",
    "                {sample_messages}\n",
    "                \n",
    "                Return a list of unique customer issues in JSON format with the issue type and description.\n",
    "                \"\"\"\n",
    "                \n",
    "                try:\n",
    "                    issues_response = self._safe_llm_call(issue_prompt)\n",
    "                    issues = json.loads(issues_response)\n",
    "                    \n",
    "                    for i, issue in enumerate(issues):\n",
    "                        issue_type = issue.get('type', f\"Unknown{i}\")\n",
    "                        description = issue.get('description', '')\n",
    "                        \n",
    "                        session.run(\"\"\"\n",
    "                            CREATE (i:CustomerIssue {\n",
    "                                id: $id,\n",
    "                                type: $type,\n",
    "                                description: $description,\n",
    "                                entity_type: 'CustomerIssue'\n",
    "                            })\n",
    "                        \"\"\", id=str(issue_type), type=str(issue_type), description=str(description))\n",
    "                except:\n",
    "                    logger.warning(\"Failed to create customer issues from chat data\")\n",
    "            \n",
    "            logger.info(\"Created basic fallback schema\")\n",
    "\n",
    "    def _safe_llm_call(self, prompt: str) -> str:\n",
    "        \"\"\"Make LLM API call with rate limiting\"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_api_call\n",
    "        if time_since_last < self.min_delay:\n",
    "            time.sleep(self.min_delay - time_since_last)\n",
    "        \n",
    "        try:\n",
    "            response = self.chat.send_message(prompt).text\n",
    "            self.last_api_call = time.time()\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in LLM call: {str(e)}\")\n",
    "            return \"{}\"  # Return empty JSON as fallback\n",
    "\n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        \"\"\"Process user query through Graph RAG pipeline\"\"\"\n",
    "        try:\n",
    "            # Step a: Use LLM to understand query and transform to graph query pattern\n",
    "            cypher_query = self._query_to_cypher(query)\n",
    "            logger.info(f\"Generated Cypher query: {cypher_query}\")\n",
    "            \n",
    "            # Step b: Execute query on knowledge graph\n",
    "            graph_results = self._execute_knowledge_graph_query(cypher_query)\n",
    "            logger.info(f\"Found {len(graph_results)} results in knowledge graph\")\n",
    "            \n",
    "            # Step c: Use LLM to analyze and enhance graph results with additional context\n",
    "            enhanced_results = self._augment_results_with_llm(query, graph_results)\n",
    "            \n",
    "            return enhanced_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query: {str(e)}\")\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "\n",
    "    def _query_to_cypher(self, query: str) -> str:\n",
    "        \"\"\"Convert natural language query to Cypher query using LLM\"\"\"\n",
    "        try:\n",
    "            # First, get schema information\n",
    "            with self.driver.session() as session:\n",
    "                # Get node labels\n",
    "                labels = session.run(\"CALL db.labels() YIELD label RETURN collect(label) AS labels\").single()[\"labels\"]\n",
    "                \n",
    "                # Get relationship types\n",
    "                rel_types = session.run(\"CALL db.relationshipTypes() YIELD relationshipType RETURN collect(relationshipType) AS types\").single()[\"types\"]\n",
    "                \n",
    "                # Get a sample of node properties for each label\n",
    "                node_properties = {}\n",
    "                for label in labels:\n",
    "                    sample = session.run(f\"MATCH (n:{label}) RETURN n LIMIT 1\").data()\n",
    "                    if sample:\n",
    "                        props = sample[0]['n']\n",
    "                        node_properties[label] = list(props.keys())\n",
    "            \n",
    "            # Use LLM to generate Cypher query\n",
    "            cypher_prompt = f\"\"\"\n",
    "            Convert this natural language query to a Cypher query for Neo4j:\n",
    "            \n",
    "            Query: \"{query}\"\n",
    "            \n",
    "            Available node labels: {labels}\n",
    "            Available relationship types: {rel_types}\n",
    "            Node properties by label: {node_properties}\n",
    "            \n",
    "            Return ONLY the Cypher query, no explanation, no markdown, no code blocks.\n",
    "            The query should always include LIMIT to prevent excessive returns.\n",
    "            \"\"\"\n",
    "            \n",
    "            cypher_text = self._safe_llm_call(cypher_prompt)\n",
    "            \n",
    "            # Clean up the response - remove markdown code blocks if present\n",
    "            cypher_text = re.sub(r'```(?:cypher)?\\s*([\\s\\S]*?)\\s*```', r'\\1', cypher_text).strip()\n",
    "            \n",
    "            # Ensure the query has a LIMIT if RETURN is present\n",
    "            if \"RETURN\" in cypher_text.upper() and \"LIMIT\" not in cypher_text.upper():\n",
    "                cypher_text += \" LIMIT 25\"\n",
    "                \n",
    "            return cypher_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting query to Cypher: {str(e)}\")\n",
    "            # Return a simple fallback query\n",
    "            return \"MATCH (n) RETURN n LIMIT 10\"\n",
    "\n",
    "    def _execute_knowledge_graph_query(self, cypher_query: str) -> List[Dict]:\n",
    "        \"\"\"Execute query on Neo4j knowledge graph\"\"\"\n",
    "        try:\n",
    "            # Clean any markdown or code blocks that might be in the query\n",
    "            cleaned_query = re.sub(r'```(?:cypher)?\\s*([\\s\\S]*?)\\s*```', r'\\1', cypher_query).strip()\n",
    "            \n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(cleaned_query)\n",
    "                return [record.data() for record in result]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing knowledge graph query: {str(e)}\")\n",
    "            return [{\"error\": str(e)}]\n",
    "\n",
    "    def _augment_results_with_llm(self, original_query: str, graph_results: List[Dict]) -> Dict:\n",
    "        \"\"\"Augment graph results with LLM analysis to produce final answer\"\"\"\n",
    "        try:\n",
    "            # Use LLM to analyze graph results and generate answer\n",
    "            augment_prompt = f\"\"\"\n",
    "            Original query: \"{original_query}\"\n",
    "            \n",
    "            Results from knowledge graph: {graph_results}\n",
    "            \n",
    "            Based on these results from the knowledge graph:\n",
    "            1. Provide a direct answer to the original query\n",
    "            2. Include relevant context and insights from the graph results\n",
    "            3. If the results seem incomplete, explain what might be missing\n",
    "            \n",
    "            Return your response as JSON with:\n",
    "            - \"answer\": A direct, comprehensive answer to the query\n",
    "            - \"insights\": Additional insights or context from the graph data\n",
    "            - \"confidence\": Your confidence in the answer (high, medium, low)\n",
    "            \"\"\"\n",
    "            \n",
    "            augment_response = self._safe_llm_call(augment_prompt)\n",
    "            \n",
    "            try:\n",
    "                final_response = json.loads(augment_response)\n",
    "            except json.JSONDecodeError:\n",
    "                # Create a fallback response if JSON parsing fails\n",
    "                final_response = {\n",
    "                    \"answer\": f\"Based on the knowledge graph, here's what I found: {graph_results[:5] if graph_results else 'No results found.'}\",\n",
    "                    \"insights\": \"The knowledge graph provided the above information based on your query.\",\n",
    "                    \"confidence\": \"medium\"\n",
    "                }\n",
    "            \n",
    "            # Include the original results and query\n",
    "            final_response[\"query\"] = original_query\n",
    "            final_response[\"graph_results\"] = graph_results\n",
    "            \n",
    "            return final_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error augmenting results with LLM: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": \"Error processing results. Please try again.\",\n",
    "                \"error\": str(e),\n",
    "                \"query\": original_query,\n",
    "                \"graph_results\": graph_results\n",
    "            }\n",
    "\n",
    "# Main function to run the system\n",
    "def run_graph_rag(neo4j_uri, neo4j_user, neo4j_password, gemini_api_key, ecommerce_path, chat_logs_path, queries):\n",
    "    \"\"\"Run the Graph RAG system with the given parameters\"\"\"\n",
    "    try:\n",
    "        # Initialize the system\n",
    "        rag_system = GraphRAGSystem(\n",
    "            neo4j_uri=neo4j_uri,\n",
    "            neo4j_user=neo4j_user,\n",
    "            neo4j_password=neo4j_password,\n",
    "            gemini_api_key=gemini_api_key\n",
    "        )\n",
    "        \n",
    "        # Load datasets and convert to knowledge graph\n",
    "        rag_system.load_datasets(ecommerce_path, chat_logs_path)\n",
    "        \n",
    "        # Process queries\n",
    "        results = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            logger.info(f\"Processing query: {query}\")\n",
    "            query_result = rag_system.process_query(query)\n",
    "            results[query] = query_result\n",
    "            logger.info(f\"Query result: {query_result.get('answer', 'No answer generated')}\")\n",
    "            time.sleep(2)  # Add delay between queries\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error running Graph RAG system: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    NEO4J_URI = \"bolt://localhost:7687\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"pulseinsights\"\n",
    "    GEMINI_API_KEY = \"AIzaSyCJ7XjljkXJHEPvYOMONw2BDoP4qGJ9NIY\"\n",
    "    \n",
    "    # File paths\n",
    "    ECOMMERCE_DATA_PATH = \"Amazon Sale Report.csv\"\n",
    "    CHAT_LOGS_PATH = \"pulseun.csv\"\n",
    "    \n",
    "    # Example queries\n",
    "    QUERIES = [\n",
    "        \"Give 5 names of products which sold last by date present in dataset?\"\n",
    "    ]\n",
    "    \n",
    "    # Run the system\n",
    "    results = run_graph_rag(\n",
    "        NEO4J_URI, \n",
    "        NEO4J_USER, \n",
    "        NEO4J_PASSWORD, \n",
    "        GEMINI_API_KEY,\n",
    "        ECOMMERCE_DATA_PATH,\n",
    "        CHAT_LOGS_PATH,\n",
    "        QUERIES\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== RESULTS ===\")\n",
    "    for query, result in results.items():\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"Answer: {result.get('answer', 'No answer generated')}\")\n",
    "        print(f\"Insights: {result.get('insights', 'No insights generated')}\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll explain step by step what's happening in this GraphRAGSystem code:\n",
    "\n",
    "Class Initialization:\n",
    "\n",
    "The system connects to a Neo4j graph database and sets up Gemini LLM (Google's large language model)\n",
    "It establishes connections with proper authentication and configures API parameters\n",
    "\n",
    "\n",
    "Data Loading:\n",
    "\n",
    "Loads two datasets: e-commerce data and chat logs from CSV files\n",
    "Prepares this data for conversion into a knowledge graph\n",
    "\n",
    "\n",
    "Knowledge Graph Creation:\n",
    "\n",
    "Clears existing database and sets up indices for performance\n",
    "Uses LLM to discover entities (nodes) and relationships in the data\n",
    "Processes e-commerce data to create entity nodes\n",
    "Processes chat data to extract entities from messages\n",
    "Creates relationships between entities based on patterns discovered by the LLM\n",
    "\n",
    "\n",
    "Knowledge Graph Enrichment:\n",
    "\n",
    "Analyzes the graph to discover additional insights\n",
    "Uses LLM to suggest new relationships and patterns\n",
    "Executes these suggestions to enhance the knowledge graph\n",
    "Has a fallback schema creation if the LLM approach fails\n",
    "\n",
    "\n",
    "Query Processing:\n",
    "\n",
    "Converts natural language queries to Cypher (Neo4j's query language) using LLM\n",
    "Executes these Cypher queries on the knowledge graph\n",
    "Gets results from the graph database\n",
    "Uses LLM again to analyze these results and enhance them with context\n",
    "\n",
    "\n",
    "Result Generation:\n",
    "\n",
    "Returns a JSON response with answers, insights, and confidence levels\n",
    "Includes original query and graph results for reference\n",
    "\n",
    "\n",
    "Main Execution:\n",
    "\n",
    "Sets up configuration parameters (database URI, credentials, API keys)\n",
    "Specifies file paths for datasets\n",
    "Defines example queries to process\n",
    "Runs the system and prints results\n",
    "\n",
    "\n",
    "\n",
    "This is effectively a RAG (Retrieval-Augmented Generation) system that uses a graph database instead of vector embeddings, allowing for more complex relationship-based queries and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
